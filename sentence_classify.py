# -*- coding: utf-8 -*-
"""chatbot_20180831_release.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FNa93tpyAYN14xnEDoYD2OeDsrUCvs4q

# 合コン50音チャットの作り方

## データを作成する
- 50音を決定する
- TwitterAPIでその言葉がリプライに使われている元のツイートを取得  
  -> 無理そうなので、とりあえず適当にツイートを集めて50音で反応する
- 可能であれば類義語辞典かWord2Vecを使って似た言葉に対するリプライについても取得  
  -> 本来であればTwitterの中からWord2Vecを使って類義語抽出後に検索したい
- とってきたツイートを形態素解析して特徴語を抽出
  -> 辞書に存在しない単語を補うためにWord2Vecの学習済みモデルをimportして
  -> MecabじゃなくてJUMAN++も検討してみたが、時間がかかりすぎそう
- データ、それに対する50音の番号を正解として組み合わせる

## 学習
- 分類でやるのか次元削減でやるのか迷うけど、とりあえずは50音に分類するので分類で。
- skflowでKerasを使う。
- 活性化関数とOptimizerは調べながら決定する

## UI(Djangoで作成)
- 自分を登録
- 相手を登録
- チャット画面
- 学習モード
 - 質問をする
 - 回答する
 - ×の場合、正解を与える
- BotUIを使って表示

## 残タスク

- 文章と返答のラベルが貼られた学習済データを作成

- 現在の辞書にない単語の場合に類似語の中に存在しないか判定する処理  
    ->Word2Vecの学習済みモデル(可能であれば竹中さんにもらう)のimportを行う  
    ->Word2Vecは単語辞書のなかからベクトルをone-hotで持ってくる(該当単語の一行抜き出す)  
    ->判定した中に単語がなければ,Word2Vecので近い単語を取得(most_similarの中とかにあるか)を確認して、似ている割合によって置き換える  

- 学習データを入力ごとに取得してCSVに書き込む処理

- 複数モデルおよび条件でのの比較
  ->隠れ層、ニューロン数の調整  
  ->活性化関数の調整  
  ->sckit-learn,skflow,tensorflow,chainerあたりで比較

- 画面側でチャットボットの表示  
  -> 学習モデルの読み込みを追加すればうまくいけそう  
  -> デバッグモードと学習モードの作成  
  -> JSを外部ファイルに 
  
## アドバンスド  
- もし普通のチャットボットを作るのであれば、単語だけ今までの返答と同じ位置にあるベクトルを返すようにするとよいかも。
- 質問とかであれば転置変換を行って検索した結果を作成するのでもよいかも。

# 必要なファイル類のアップロード
"""

#---データ作成---#
#50音データ読み込み
from google.colab import files
import pandas as pd

#reply.csvのインポート
uploaded = files.upload()

#data.csvのインポート
files.upload()

# coding: utf-8

#必要なパッケージのinstall
!apt install aptitude
!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y
!pip install mecab-python3
!pip install gensim

#---ファイルアップロードが遅すぎるのでGoogleDriveから取り込み---#

import google.colab
import googleapiclient.discovery
import googleapiclient.http

google.colab.auth.authenticate_user()
drive_service = googleapiclient.discovery.build('drive', 'v3')

upload_filename = 'mix_data.zip'

file_list = drive_service.files().list(q="name='" + upload_filename + "'").execute().get('files')

# ファイル ID を取得します。
file_id = None
for file in file_list:
  if file.get('name') == upload_filename:
    file_id = file.get('id')
    break

if file_id is None:
  # ファイル ID を取得できなかった場合はエラーメッセージを出力します。
  print(upload_filename + ' が見つかりません.')
else:
  # colab 環境へファイルをアップロードします。
  with open(upload_filename, 'wb') as f:
    request = drive_service.files().get_media(fileId=file_id)
    media = googleapiclient.http.MediaIoBaseDownload(f, request)

    done = False
    while not done:
      progress_status, done = media.next_chunk()
      print(100*progress_status.progress(), end="")
      print("%完了")

  print('GoogleドライブからColab環境へのファイル取り込みが完了しました.')

!unzip mix_data.zip
!ls

"""# Twitterでデータ集めようと思ったけど断念してアメブロからスクレイピング"""

#--TwitterAPI--#
### 申請中 ###
### とりあえずデータを集めてラベルを張り付ける　###
### twitter ###
"""
SEARCH_URL = "https://api.twitter.com/1.1/search/tweets.json"


for i in reply_df["Words"]:
  params ={"q":reply_df[i]}
  res = twitter.get(SEARCH_URL, params = params)
  if res.status_code == 200 :
    search_result = json.loads(res.text)
"""

"""# Word2Vecに必要なモジュール等を追加"""

!pip install gensim

import gensim
wtv = gensim.models.Word2Vec.load('mix_data')

# 似た単語で置き換えてCSVデータ追加
results = wtv.wv.most_similar(positive=['汚い','ポジティブ'],negative=['ネガティブ'])
print(results)
result_noun = wtv.wv.most_similar(positive=['汚い'])
print(result_noun)

"""# 形態素解析の準備"""

#--形態素解析してTF-IDFで特徴語を辞書化--#
import MeCab
import copy

#必要なメンバ
text=[]
classify=[]
words=[]
new_words= []
dict=[]
cnt = 0

#辞書の読込み
tagger = MeCab.Tagger("-Ochasen")

#Mecabで単語単位に分けるクラス
class WordDividor:
    INDEX_CATEGORY = 0 #品詞が入っている
    INDEX_ROOT_FORM = 6 #形態素の基本形が入っている(ex.食べ⇒「食べる」)
    TARGET_CATEGORIES = ["名詞","動詞","形容詞","連体詞","副詞"]

    def __init__(self, dictionary="mecabrc"):
        self.dictionary = dictionary
        self.tagger = MeCab.Tagger(self.dictionary)
        
    def extract_words(self, text):
        if not text:
            return []

        words = []
        
        node = self.tagger.parseToNode(text)
        #node.featureで表層系の形態素解析をしたカンマ区切りの値が戻ってくる(ex.「名詞,副詞可能,*,*,*,*,今日,キョウ,キョー」)
        #名詞だけになっているから文章全体を読み込ませる
        while node:
            features = node.feature.split(',')
           
            #定義リストに存在する品詞で基本形がある場合とない場合に分類。ない場合には表層型を追加する。
            #pythonではクラスの値を呼び出すときは定数も変数もメソッドもself(実態はWordDividerクラスの先頭ポインタ)
            if features[self.INDEX_CATEGORY] in self.TARGET_CATEGORIES:
                if features[self.INDEX_ROOT_FORM] == "*":
                    words.append(node.surface)
                else:
                    words.append(features[self.INDEX_ROOT_FORM])
            node = node.next
        
        self.increase_chat_data(words)
        return words



    def increase_chat_data(self,words):
        new_data = []
        pre_data = copy.deepcopy(words)

        for i,word in enumerate(words):
            try:
                results = wtv.wv.most_similar(positive=[word,'ポジティブ'],negative=['ネガティブ'])
                for result in results:
                    cmp = self.compare_similar_word(pre_data[i],result[0])
                    if result[1] > 0.6 and cmp == True:
                      pre_data[i] = result[0]
                      new_data.append(copy.deepcopy(pre_data))
                    else:
                      break

            except:
                pass
        new_words.append(new_data)
            
            
    def compare_similar_word(self,pre_word,new_word):
    
        INDEX_PART = 0 # 品詞
        INDEX_PARTICAL_CLASSIFY_1 = 1 #品詞細分類１
        if not (pre_word or new_word):
            return False

        pre = tagger.parseToNode(pre_word)
        pre_features = pre.next.feature.split(',')

        new = tagger.parseToNode(new_word)
        new_features = new.next.feature.split(',')

        if (pre_features[INDEX_PART] == new_features[INDEX_PART]) and \
               (pre_features[INDEX_PARTICAL_CLASSIFY_1] == new_features[INDEX_PARTICAL_CLASSIFY_1]) :
            return True

        return False

# 似た単語で置き換えてCSVデータ追加
results = wtv.wv.most_similar(positive=['汚い','ポジティブ'],negative=['ネガティブ'])
print(results)
result_noun = wtv.wv.most_similar(positive=['汚い'])
print(result_noun)

wd = WordDividor()
wd.extract_words("私はごはんを食べます")
wd.extract_words("これはとても楽しいです")
wd.extract_words("信じられない")
print(new_words)

"""# 文章のベクトル化

## 形態素解析で単語単位に分解

## 単語の辞書リストを作成

##  ベクトル化
"""

#Word2vecを使って辞書の単語を増やす#

#--Modelの作成--#
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.externals import joblib
import tensorflow.contrib.learn as skflow
import itertools

#ファイルから読み込み
X=None # 学習データ
Y=None # ラベルデータ

reply_df = pd.read_csv('reply.csv')
df = pd.read_csv('data.csv')
sentence = df["sentence"]
label = df["label"]
add_sentence = []
add_label  = []
add_data = []

#sentenceを単語に分解(analyzerにextract_wordsの戻り値を設定)
wd = WordDividor()

#word2Vecでデータかさ増し
for i,line in enumerate(sentence):
    word_list = wd.extract_words(line)
    for new_word in new_words:
        try:
          for nw in new_word:
            n = ''.join(nw)
            add_data.append([n,int(label[i])])
          new_words  = []
        except:
          pass
print(add_data)

add_df = pd.DataFrame(add_data,columns=['sentence', 'label'])
df = df.append(add_df)

print(df)

sentence = df["sentence"]
label = df["label"]

#ラベル(分類を作る)
le = LabelEncoder()
classify_label=le.fit_transform(label)
#print(classify_label)

#ngram(テキスト分割方法)->テキストを辞書型の単語に分割
cv = TfidfVectorizer(analyzer=wd.extract_words)
#sentence _add = sentence + new_data
classify_dictionary = cv.fit_transform(sentence) #文章ごとに辞書化してラベルはり
#classify_dictionary = TfidfVectorizer(vocabulary)
print(classify_dictionary)

#単語の出現を辞書の番号でカウント(通常のベクトル→辞書に対する文章ごとの単語の出現率でいろんな文章に共通のものの特徴率を低くしたもの)
#今回は辞書型で学習させたが、numpyarrayに変換すれば別の分類器でも使える？Kerasにこの配列とラベルを送り込んでみたい。
#appearance = classify_dictionary.toarray()
#print(appearance)



"""# モデルの作成(Scikit-learn)"""

#カテゴリを数値に変更
model_sklearn = MLPClassifier(hidden_layer_sizes=(200,300,),early_stopping=True,max_iter=300,activation='relu',solver='adam')
data = model_sklearn.fit(classify_dictionary,classify_label)

"""
classifier = skflow.TensorFlowRNNClassifier(rnn_size=EMBEDDING_SIZE, 
    n_classes=15, cell_type='gru', input_op_fn=input_op_fn,
    num_layers=1, bidirectional=False, sequence_length=None,
    steps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True)

model = classifier.fit(classify_dictionary,classify_label)
"""

# --- modelの保存と呼び出し --- #
joblib.dump(model_sklearn, 'model.pkl')
#joblib.dump(appearance,'appear.pkl')
joblib.dump(le, 'le.pkl')

m = joblib.load('model.pkl')
#a = joblib.load('appear.pkl')
le = joblib.load('le.pkl')

!7z a chatbot.7z le.pkl model.pkl

from google.colab import files
files.download('chatbot.7z')

"""# 予測"""

#予測
a="それってすごい！"

text=[]
text.append(a)
new_data = cv.transform(text)
classify=m.predict(new_data)
compatible_class = le.inverse_transform(classify)

print(compatible_class)

for (c,x) in  enumerate(m.predict_proba(new_data)[0]):
    print(":".join([str(c), str(x)]))
    
result = reply_df.query('Label == '+ str(compatible_class))
print(result['Words'])

"""# モデルの作成(Keras)"""

import keras
import tensorflow as tf
import numpy as np
import numpy as np

#--モデルの作成--#
ips = classify_dictionary.shape[1] # 　辞書の単語数をshapeに変換
l = np.unique(classify_label) #ラベルの個数を取得
model_keras = keras.Sequential()
#隠れ層(input_shapeは辞書の長さの変数、アウトプットは使われたラベルの個数とする)
model_keras.add(keras.layers.Dense(32, input_shape=(ips,)))
model_keras.add(keras.layers.Dense(16, activation=tf.nn.relu))
model_keras.add(keras.layers.Dense(l.shape[0], activation=tf.nn.sigmoid))
model_keras.summary()


#--モデルのコンパイル--#
model_keras.compile(optimizer=tf.train.AdamOptimizer(), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
#--学習--#
history = model_keras.fit(classify_dictionary,
                    classify_label,
                    epochs=300,
                    batch_size=512,
                    #validation_data=(x_val, y_val),
                    verbose=1)

#--学習の状況を表示--#
#results = model.evaluate(test_data, test_labels)
#print(results)

#from keras.utils import plot_model
#plot_model(model, to_file='model.png')

#-- 正答率変動を表示 --#
history_dict = history.history
history_dict.keys()

import matplotlib.pyplot as plt

acc = history.history['acc']
loss = history.history['loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.clf()   # clear figure
acc_values = history_dict['acc']

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

model_keras.save_weights('model_keras')
files.download('model_keras')

"""# 予測"""

#keras 予測
a="今日は花金だぜ"

text=[]
text.append(a)
new_data = cv.transform(text)
print(new_data)
# 予測に
classify=model_keras.predict_classes(new_data)
compatible_class = le.inverse_transform(classify)

for (c,x) in  enumerate(model_keras.predict_proba(new_data)[0]):
    print(":".join([str(c), str(x)]))
    
result = reply_df.query('Label == '+ str(compatible_class))
print(result['Words'])

"""# Keras(モデルの構成を変えてみた)"""

import keras
import tensorflow as tf
import numpy as np



#--モデルの作成--#
l = np.unique(classify_label)
model_keras2 = keras.Sequential()
#隠れ層(input_shapeは辞書の長さの変数、アウトプットは使われたラベルの個数とする)
model_keras2.add(keras.layers.Dense(128, input_shape=(classify_dictionary.shape[1],)))
model_keras2.add(keras.layers.Dense(128, activation=tf.nn.relu))
model_keras2.add(keras.layers.Dense(l.shape[0], activation=tf.nn.sigmoid))
model_keras2.summary()


#--モデルのコンパイル--#
model_keras2.compile(optimizer=tf.train.AdamOptimizer(), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
#--学習--#
history = model_keras2.fit(classify_dictionary,
                    classify_label,
                    epochs=100,
                    batch_size=256,
                    #validation_data=(x_val, y_val),
                    verbose=1)

#-- 正答率変動を表示 --#
history_dict = history.history
history_dict.keys()

import matplotlib.pyplot as plt

acc = history.history['acc']
loss = history.history['loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.clf()   # clear figure
acc_values = history_dict['acc']

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

"""# Keras(活性化関数の変更)"""

import keras
import tensorflow as tf
import numpy as np



#--モデルの作成--#
l = np.unique(classify_label)
model_keras4 = keras.Sequential()
#隠れ層(input_shapeは辞書の長さの変数、アウトプットは使われたラベルの個数とする)
model_keras4.add(keras.layers.Dense(128, input_shape=(classify_dictionary.shape[1],),activation='softmax'))
model_keras4.add(keras.layers.Dense(128, activation=tf.nn.relu))
model_keras4.add(keras.layers.Dense(l.shape[0], activation=tf.nn.sigmoid))
model_keras4.summary()


#--モデルのコンパイル--#
model_keras4.compile(optimizer=tf.train.AdamOptimizer(), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
#--学習--#
history = model_keras4.fit(classify_dictionary,
                    classify_label,
                    epochs=300,
                    batch_size=256,
                    #validation_data=(x_val, y_val),
                    verbose=1)

#-- 正答率変動を表示 --#
history_dict = history.history
history_dict.keys()

import matplotlib.pyplot as plt

acc = history.history['acc']
loss = history.history['loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.clf()   # clear figure
acc_values = history_dict['acc']

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

"""# Keras(活性化関数を変えてみた:tanh)"""

import keras
import tensorflow as tf
import numpy as np



#--モデルの作成--#
l = np.unique(classify_label)
model_keras5 = keras.Sequential()
#隠れ層(input_shapeは辞書の長さの変数、アウトプットは使われたラベルの個数とする)
model_keras5.add(keras.layers.Dense(128, input_shape=(classify_dictionary.shape[1],),activation='tanh'))
model_keras5.add(keras.layers.Dense(128, activation=tf.nn.relu))
model_keras5.add(keras.layers.Dense(l.shape[0], activation=tf.nn.sigmoid))
model_keras5.summary()


#--モデルのコンパイル--#
model_keras5.compile(optimizer=tf.train.AdamOptimizer(), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
#--学習--#
history = model_keras5.fit(classify_dictionary,
                    classify_label,
                    epochs=100,
                    batch_size=256,
                    #validation_data=(x_val, y_val),
                    verbose=1)

#-- 正答率変動を表示 --#
history_dict = history.history
history_dict.keys()

import matplotlib.pyplot as plt

acc = history.history['acc']
loss = history.history['loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.clf()   # clear figure
acc_values = history_dict['acc']

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

import keras
import tensorflow as tf
import numpy as np



#--モデルの作成--#
l = np.unique(classify_label)
model = keras.Sequential()
#隠れ層(input_shapeは辞書の長さの変数、アウトプットは使われたラベルの個数とする)
model.add(keras.layers.Dense(128, input_shape=(classify_dictionary.shape[1],),activation='tanh'))
model.add(keras.layers.LSTM(32))
model.add(keras.layers.Dense(l.shape[0], activation=tf.nn.sigmoid))
model.summary()


#--モデルのコンパイル--#
model.compile(optimizer=tf.train.AdamOptimizer(), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
#--学習--#
history = model.fit(classify_dictionary,
                    classify_label,
                    epochs=100,
                    batch_size=256,
                    #validation_data=(x_val, y_val),
                    verbose=1)